17/08/10 05:36:00 INFO batch.Application$: [Ljava.lang.String;@12f41634
17/08/10 05:36:00 INFO batch.Application$: env.json
17/08/10 05:36:00 INFO batch.Application$: config.json
17/08/10 05:36:01 INFO batch.Application$: params validation pass
17/08/10 05:36:01 INFO spark.SparkContext: Running Spark version 1.6.0
17/08/10 05:36:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/08/10 05:36:02 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '-Dspark.driver.port=53411').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
17/08/10 05:36:02 WARN spark.SparkConf: Setting 'spark.executor.extraJavaOptions' to '-Dspark.driver.port=53411' as a work-around.
17/08/10 05:36:02 WARN spark.SparkConf: Setting 'spark.driver.extraJavaOptions' to '-Dspark.driver.port=53411' as a work-around.
17/08/10 05:36:02 INFO spark.SecurityManager: Changing view acls to: root
17/08/10 05:36:02 INFO spark.SecurityManager: Changing modify acls to: root
17/08/10 05:36:02 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root)
17/08/10 05:36:02 INFO util.Utils: Successfully started service 'sparkDriver' on port 53411.
17/08/10 05:36:03 INFO slf4j.Slf4jLogger: Slf4jLogger started
17/08/10 05:36:03 INFO Remoting: Starting remoting
17/08/10 05:36:03 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@172.17.0.2:53412]
17/08/10 05:36:03 INFO util.Utils: Successfully started service 'sparkDriverActorSystem' on port 53412.
17/08/10 05:36:03 INFO spark.SparkEnv: Registering MapOutputTracker
17/08/10 05:36:03 INFO spark.SparkEnv: Registering BlockManagerMaster
17/08/10 05:36:03 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-15f50a6e-d1d4-49a0-b4eb-ed77032118ff
17/08/10 05:36:03 INFO storage.MemoryStore: MemoryStore started with capacity 511.1 MB
17/08/10 05:36:03 INFO spark.SparkEnv: Registering OutputCommitCoordinator
17/08/10 05:36:03 INFO server.Server: jetty-8.y.z-SNAPSHOT
17/08/10 05:36:03 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040
17/08/10 05:36:03 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
17/08/10 05:36:03 INFO ui.SparkUI: Started SparkUI at http://172.17.0.2:4040
17/08/10 05:36:03 INFO spark.HttpFileServer: HTTP File server directory is /tmp/spark-9a7a7407-9038-4a80-bba5-dc8b7ea7dcfa/httpd-a3779ab6-54ec-4181-8c30-f3369889cce1
17/08/10 05:36:03 INFO spark.HttpServer: Starting HTTP Server
17/08/10 05:36:03 INFO server.Server: jetty-8.y.z-SNAPSHOT
17/08/10 05:36:03 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:33112
17/08/10 05:36:03 INFO util.Utils: Successfully started service 'HTTP file server' on port 33112.
17/08/10 05:36:03 INFO spark.SparkContext: Added JAR file:/root/huhang/griffin-sample/accuracy-sample/measure-0.1.5-incubating.jar at http://172.17.0.2:33112/jars/measure-0.1.5-incubating.jar with timestamp 1502343363745
17/08/10 05:36:04 INFO client.RMProxy: Connecting to ResourceManager at sandbox/172.17.0.2:8032
17/08/10 05:36:04 INFO yarn.Client: Requesting a new application from cluster with 1 NodeManagers
17/08/10 05:36:04 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
17/08/10 05:36:04 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
17/08/10 05:36:04 INFO yarn.Client: Setting up container launch context for our AM
17/08/10 05:36:04 INFO yarn.Client: Setting up the launch environment for our AM container
17/08/10 05:36:04 INFO yarn.Client: Preparing resources for our AM container
17/08/10 05:36:05 INFO yarn.Client: Source and destination file systems are the same. Not copying hdfs:/home/spark_lib/spark-assembly-1.6.0-hadoop2.6.0.jar
17/08/10 05:36:05 INFO yarn.Client: Source and destination file systems are the same. Not copying hdfs:/home/spark_conf/hive-site.xml
17/08/10 05:36:05 INFO yarn.Client: Uploading resource file:/tmp/spark-9a7a7407-9038-4a80-bba5-dc8b7ea7dcfa/__spark_conf__6540274617259251624.zip -> hdfs://sandbox:9000/user/root/.sparkStaging/application_1500434006041_0046/__spark_conf__6540274617259251624.zip
17/08/10 05:36:05 INFO spark.SecurityManager: Changing view acls to: root
17/08/10 05:36:05 INFO spark.SecurityManager: Changing modify acls to: root
17/08/10 05:36:05 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root)
17/08/10 05:36:05 INFO yarn.Client: Submitting application 46 to ResourceManager
17/08/10 05:36:05 INFO impl.YarnClientImpl: Submitted application application_1500434006041_0046
17/08/10 05:36:06 INFO yarn.Client: Application report for application_1500434006041_0046 (state: ACCEPTED)
17/08/10 05:36:06 INFO yarn.Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1502343365339
	 final status: UNDEFINED
	 tracking URL: http://sandbox:8088/proxy/application_1500434006041_0046/
	 user: root
17/08/10 05:36:07 INFO yarn.Client: Application report for application_1500434006041_0046 (state: ACCEPTED)
17/08/10 05:36:08 INFO yarn.Client: Application report for application_1500434006041_0046 (state: ACCEPTED)
17/08/10 05:36:08 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(null)
17/08/10 05:36:08 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> sandbox, PROXY_URI_BASES -> http://sandbox:8088/proxy/application_1500434006041_0046), /proxy/application_1500434006041_0046
17/08/10 05:36:08 INFO ui.JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
17/08/10 05:36:09 INFO yarn.Client: Application report for application_1500434006041_0046 (state: RUNNING)
17/08/10 05:36:09 INFO yarn.Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 172.17.0.2
	 ApplicationMaster RPC port: 0
	 queue: default
	 start time: 1502343365339
	 final status: UNDEFINED
	 tracking URL: http://sandbox:8088/proxy/application_1500434006041_0046/
	 user: root
17/08/10 05:36:09 INFO cluster.YarnClientSchedulerBackend: Application application_1500434006041_0046 has started running.
17/08/10 05:36:09 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35444.
17/08/10 05:36:09 INFO netty.NettyBlockTransferService: Server created on 35444
17/08/10 05:36:09 INFO storage.BlockManagerMaster: Trying to register BlockManager
17/08/10 05:36:09 INFO storage.BlockManagerMasterEndpoint: Registering block manager 172.17.0.2:35444 with 511.1 MB RAM, BlockManagerId(driver, 172.17.0.2, 35444)
17/08/10 05:36:09 INFO storage.BlockManagerMaster: Registered BlockManager
17/08/10 05:36:13 INFO cluster.YarnClientSchedulerBackend: Registered executor NettyRpcEndpointRef(null) (sandbox:35129) with ID 1
17/08/10 05:36:13 INFO storage.BlockManagerMasterEndpoint: Registering block manager sandbox:44160 with 511.1 MB RAM, BlockManagerId(1, sandbox, 44160)
17/08/10 05:36:14 INFO cluster.YarnClientSchedulerBackend: Registered executor NettyRpcEndpointRef(null) (sandbox:35130) with ID 2
17/08/10 05:36:14 INFO storage.BlockManagerMasterEndpoint: Registering block manager sandbox:60885 with 511.1 MB RAM, BlockManagerId(2, sandbox, 60885)
17/08/10 05:36:14 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
17/08/10 05:36:14 INFO hive.HiveContext: Initializing execution hive, version 1.2.1
17/08/10 05:36:15 INFO client.ClientWrapper: Inspected Hadoop version: 2.6.0
17/08/10 05:36:15 INFO client.ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0
17/08/10 05:36:15 INFO metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/08/10 05:36:15 INFO metastore.ObjectStore: ObjectStore, initialize called
17/08/10 05:36:15 INFO DataNucleus.Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/08/10 05:36:15 INFO DataNucleus.Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/08/10 05:36:15 WARN DataNucleus.Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/08/10 05:36:16 WARN DataNucleus.Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/08/10 05:36:17 INFO metastore.ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/08/10 05:36:18 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/08/10 05:36:18 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/08/10 05:36:18 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/08/10 05:36:18 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/08/10 05:36:19 INFO metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/08/10 05:36:19 INFO metastore.ObjectStore: Initialized ObjectStore
17/08/10 05:36:19 WARN metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/08/10 05:36:19 WARN metastore.ObjectStore: Failed to get database default, returning NoSuchObjectException
17/08/10 05:36:19 INFO metastore.HiveMetaStore: Added admin role in metastore
17/08/10 05:36:19 INFO metastore.HiveMetaStore: Added public role in metastore
17/08/10 05:36:19 INFO metastore.HiveMetaStore: No user is added in admin role, since config is empty
17/08/10 05:36:19 INFO metastore.HiveMetaStore: 0: get_all_databases
17/08/10 05:36:19 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	
17/08/10 05:36:19 INFO metastore.HiveMetaStore: 0: get_functions: db=default pat=*
17/08/10 05:36:19 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/08/10 05:36:19 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/08/10 05:36:19 INFO session.SessionState: Created HDFS directory: /tmp/hive-root/root/07848229-84aa-4fda-adf4-8aa7c437b3c9
17/08/10 05:36:19 INFO session.SessionState: Created local directory: /tmp/root/07848229-84aa-4fda-adf4-8aa7c437b3c9
17/08/10 05:36:19 INFO session.SessionState: Created HDFS directory: /tmp/hive-root/root/07848229-84aa-4fda-adf4-8aa7c437b3c9/_tmp_space.db
17/08/10 05:36:19 INFO hive.HiveContext: default warehouse location is /user/hive/warehouse
17/08/10 05:36:19 INFO hive.HiveContext: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17/08/10 05:36:19 INFO client.ClientWrapper: Inspected Hadoop version: 2.6.0
17/08/10 05:36:19 INFO client.ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0
17/08/10 05:36:20 INFO hive.metastore: Trying to connect to metastore with URI thrift://sandbox:9083
17/08/10 05:36:20 INFO hive.metastore: Connected to metastore.
17/08/10 05:36:20 INFO session.SessionState: Created HDFS directory: /tmp/hive-root/root/d61f1208-80e9-425d-ae4b-cf97dcaef343
17/08/10 05:36:20 INFO session.SessionState: Created local directory: /tmp/root/d61f1208-80e9-425d-ae4b-cf97dcaef343
17/08/10 05:36:20 INFO session.SessionState: Created HDFS directory: /tmp/hive-root/root/d61f1208-80e9-425d-ae4b-cf97dcaef343/_tmp_space.db
17/08/10 05:36:20 INFO persist.LoggerPersist: accu-sample start
17/08/10 05:36:20 INFO avro.AvroRelation: Listing hdfs://sandbox:9000/griffin/data/users_info_src.avro on driver
17/08/10 05:36:21 INFO storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 61.9 KB, free 61.9 KB)
17/08/10 05:36:21 INFO storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 19.6 KB, free 81.5 KB)
17/08/10 05:36:21 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.17.0.2:35444 (size: 19.6 KB, free: 511.1 MB)
17/08/10 05:36:21 INFO spark.SparkContext: Created broadcast 0 from flatMap at AvroDataConnector.scala:66
17/08/10 05:36:21 INFO storage.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 212.9 KB, free 294.3 KB)
17/08/10 05:36:21 INFO storage.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.6 KB, free 313.9 KB)
17/08/10 05:36:21 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.17.0.2:35444 (size: 19.6 KB, free: 511.1 MB)
17/08/10 05:36:21 INFO spark.SparkContext: Created broadcast 1 from hadoopFile at AvroRelation.scala:121
17/08/10 05:36:21 INFO avro.AvroRelation: Listing hdfs://sandbox:9000/griffin/data/users_info_target.avro on driver
17/08/10 05:36:21 INFO storage.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 212.9 KB, free 526.8 KB)
17/08/10 05:36:21 INFO storage.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 19.6 KB, free 546.4 KB)
17/08/10 05:36:21 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.17.0.2:35444 (size: 19.6 KB, free: 511.1 MB)
17/08/10 05:36:21 INFO spark.SparkContext: Created broadcast 2 from flatMap at AvroDataConnector.scala:66
17/08/10 05:36:21 INFO storage.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 212.9 KB, free 759.3 KB)
17/08/10 05:36:21 INFO storage.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 19.6 KB, free 778.9 KB)
17/08/10 05:36:21 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.17.0.2:35444 (size: 19.6 KB, free: 511.0 MB)
17/08/10 05:36:21 INFO spark.SparkContext: Created broadcast 3 from hadoopFile at AvroRelation.scala:121
17/08/10 05:36:21 INFO mapred.FileInputFormat: Total input paths to process : 1
17/08/10 05:36:21 INFO mapred.FileInputFormat: Total input paths to process : 1
17/08/10 05:36:21 INFO spark.SparkContext: Starting job: aggregate at AccuracyCore.scala:70
17/08/10 05:36:21 INFO scheduler.DAGScheduler: Registering RDD 18 (map at BatchAccuracyAlgo.scala:146)
17/08/10 05:36:21 INFO scheduler.DAGScheduler: Registering RDD 19 (map at BatchAccuracyAlgo.scala:147)
17/08/10 05:36:21 INFO scheduler.DAGScheduler: Got job 0 (aggregate at AccuracyCore.scala:70) with 2 output partitions
17/08/10 05:36:21 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (aggregate at AccuracyCore.scala:70)
17/08/10 05:36:21 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 0, ShuffleMapStage 1)
17/08/10 05:36:22 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 0, ShuffleMapStage 1)
17/08/10 05:36:22 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[18] at map at BatchAccuracyAlgo.scala:146), which has no missing parents
17/08/10 05:36:22 INFO storage.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 13.5 KB, free 792.4 KB)
17/08/10 05:36:22 INFO storage.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.4 KB, free 798.8 KB)
17/08/10 05:36:22 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.17.0.2:35444 (size: 6.4 KB, free: 511.0 MB)
17/08/10 05:36:22 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1006
17/08/10 05:36:22 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[18] at map at BatchAccuracyAlgo.scala:146)
17/08/10 05:36:22 INFO cluster.YarnScheduler: Adding task set 0.0 with 2 tasks
17/08/10 05:36:22 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[19] at map at BatchAccuracyAlgo.scala:147), which has no missing parents
17/08/10 05:36:22 INFO storage.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 13.5 KB, free 812.3 KB)
17/08/10 05:36:22 INFO storage.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.4 KB, free 818.7 KB)
17/08/10 05:36:22 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.17.0.2:35444 (size: 6.4 KB, free: 511.0 MB)
17/08/10 05:36:22 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1006
17/08/10 05:36:22 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[19] at map at BatchAccuracyAlgo.scala:147)
17/08/10 05:36:22 INFO cluster.YarnScheduler: Adding task set 1.0 with 2 tasks
17/08/10 05:36:22 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, sandbox, partition 0,NODE_LOCAL, 2319 bytes)
17/08/10 05:36:22 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, sandbox, partition 1,NODE_LOCAL, 2319 bytes)
17/08/10 05:36:23 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on sandbox:44160 (size: 6.4 KB, free: 511.1 MB)
17/08/10 05:36:23 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on sandbox:60885 (size: 6.4 KB, free: 511.1 MB)
17/08/10 05:36:24 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on sandbox:44160 (size: 19.6 KB, free: 511.1 MB)
17/08/10 05:36:24 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on sandbox:60885 (size: 19.6 KB, free: 511.1 MB)
17/08/10 05:36:25 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, sandbox, partition 0,NODE_LOCAL, 2322 bytes)
17/08/10 05:36:25 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 3447 ms on sandbox (1/2)
17/08/10 05:36:25 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on sandbox:44160 (size: 6.4 KB, free: 511.1 MB)
17/08/10 05:36:25 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on sandbox:44160 (size: 19.6 KB, free: 511.1 MB)
17/08/10 05:36:25 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, sandbox, partition 1,NODE_LOCAL, 2322 bytes)
17/08/10 05:36:25 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3627 ms on sandbox (2/2)
17/08/10 05:36:25 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (map at BatchAccuracyAlgo.scala:146) finished in 3.630 s
17/08/10 05:36:25 INFO scheduler.DAGScheduler: looking for newly runnable stages
17/08/10 05:36:25 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/08/10 05:36:25 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 1)
17/08/10 05:36:25 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 2)
17/08/10 05:36:25 INFO scheduler.DAGScheduler: failed: Set()
17/08/10 05:36:25 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on sandbox:60885 (size: 6.4 KB, free: 511.1 MB)
17/08/10 05:36:25 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 208 ms on sandbox (1/2)
17/08/10 05:36:25 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on sandbox:60885 (size: 19.6 KB, free: 511.1 MB)
17/08/10 05:36:25 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 159 ms on sandbox (2/2)
17/08/10 05:36:25 INFO scheduler.DAGScheduler: ShuffleMapStage 1 (map at BatchAccuracyAlgo.scala:147) finished in 3.758 s
17/08/10 05:36:25 INFO scheduler.DAGScheduler: looking for newly runnable stages
17/08/10 05:36:25 INFO scheduler.DAGScheduler: running: Set()
17/08/10 05:36:25 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 2)
17/08/10 05:36:25 INFO scheduler.DAGScheduler: failed: Set()
17/08/10 05:36:25 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[22] at map at AccuracyCore.scala:35), which has no missing parents
17/08/10 05:36:25 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
17/08/10 05:36:25 INFO storage.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 12.3 KB, free 831.0 KB)
17/08/10 05:36:25 INFO storage.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 5.2 KB, free 836.2 KB)
17/08/10 05:36:25 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.17.0.2:35444 (size: 5.2 KB, free: 511.0 MB)
17/08/10 05:36:25 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1006
17/08/10 05:36:25 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[22] at map at AccuracyCore.scala:35)
17/08/10 05:36:25 INFO cluster.YarnScheduler: Adding task set 2.0 with 2 tasks
17/08/10 05:36:25 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4, sandbox, partition 0,PROCESS_LOCAL, 2034 bytes)
17/08/10 05:36:25 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5, sandbox, partition 1,PROCESS_LOCAL, 2034 bytes)
17/08/10 05:36:25 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on sandbox:60885 (size: 5.2 KB, free: 511.1 MB)
17/08/10 05:36:25 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on sandbox:44160 (size: 5.2 KB, free: 511.1 MB)
17/08/10 05:36:25 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to sandbox:35129
17/08/10 05:36:25 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to sandbox:35130
17/08/10 05:36:25 INFO spark.MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 154 bytes
17/08/10 05:36:25 INFO spark.MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 154 bytes
17/08/10 05:36:25 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to sandbox:35130
17/08/10 05:36:25 INFO spark.MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 154 bytes
17/08/10 05:36:25 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to sandbox:35129
17/08/10 05:36:26 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 208 ms on sandbox (1/2)
17/08/10 05:36:26 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 227 ms on sandbox (2/2)
17/08/10 05:36:26 INFO scheduler.DAGScheduler: ResultStage 2 (aggregate at AccuracyCore.scala:70) finished in 0.229 s
17/08/10 05:36:26 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
17/08/10 05:36:26 INFO scheduler.DAGScheduler: Job 0 finished: aggregate at AccuracyCore.scala:70, took 4.114408 s
17/08/10 05:36:26 INFO persist.LoggerPersist: 1502343386096: calculation using time: 5582 ms
17/08/10 05:36:26 INFO persist.LoggerPersist: accu-sample result: 
match percentage: 92.0
total count: 50
miss count: 4, match count: 46
17/08/10 05:36:26 INFO persist.HdfsPersist: match percentage: 92.0
total count: 50
miss count: 4, match count: 46
17/08/10 05:36:26 INFO persist.LoggerPersist: accu-sample miss records: 
17/08/10 05:36:26 INFO spark.SparkContext: Starting job: count at LoggerPersist.scala:65
17/08/10 05:36:26 INFO scheduler.DAGScheduler: Got job 1 (count at LoggerPersist.scala:65) with 2 output partitions
17/08/10 05:36:26 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (count at LoggerPersist.scala:65)
17/08/10 05:36:26 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3, ShuffleMapStage 4)
17/08/10 05:36:26 INFO scheduler.DAGScheduler: Missing parents: List()
17/08/10 05:36:26 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[25] at map at BatchAccuracyAlgo.scala:121), which has no missing parents
17/08/10 05:36:26 INFO storage.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 14.8 KB, free 851.1 KB)
17/08/10 05:36:26 INFO storage.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.2 KB, free 857.3 KB)
17/08/10 05:36:26 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.17.0.2:35444 (size: 6.2 KB, free: 511.0 MB)
17/08/10 05:36:26 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1006
17/08/10 05:36:26 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 5 (MapPartitionsRDD[25] at map at BatchAccuracyAlgo.scala:121)
17/08/10 05:36:26 INFO cluster.YarnScheduler: Adding task set 5.0 with 2 tasks
17/08/10 05:36:26 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 6, sandbox, partition 0,PROCESS_LOCAL, 2034 bytes)
17/08/10 05:36:26 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 5.0 (TID 7, sandbox, partition 1,PROCESS_LOCAL, 2034 bytes)
17/08/10 05:36:26 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on sandbox:60885 (size: 6.2 KB, free: 511.1 MB)
17/08/10 05:36:26 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on sandbox:44160 (size: 6.2 KB, free: 511.1 MB)
17/08/10 05:36:26 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 5.0 (TID 7) in 82 ms on sandbox (1/2)
17/08/10 05:36:26 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 6) in 89 ms on sandbox (2/2)
17/08/10 05:36:26 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool 
17/08/10 05:36:26 INFO scheduler.DAGScheduler: ResultStage 5 (count at LoggerPersist.scala:65) finished in 0.090 s
17/08/10 05:36:26 INFO scheduler.DAGScheduler: Job 1 finished: count at LoggerPersist.scala:65, took 0.104815 s
17/08/10 05:36:26 INFO spark.SparkContext: Starting job: take at LoggerPersist.scala:68
17/08/10 05:36:26 INFO scheduler.DAGScheduler: Got job 2 (take at LoggerPersist.scala:68) with 1 output partitions
17/08/10 05:36:26 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (take at LoggerPersist.scala:68)
17/08/10 05:36:26 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 6, ShuffleMapStage 7)
17/08/10 05:36:26 INFO scheduler.DAGScheduler: Missing parents: List()
17/08/10 05:36:26 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[25] at map at BatchAccuracyAlgo.scala:121), which has no missing parents
17/08/10 05:36:26 INFO storage.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 15.0 KB, free 872.2 KB)
17/08/10 05:36:26 INFO storage.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 6.3 KB, free 878.5 KB)
17/08/10 05:36:26 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.17.0.2:35444 (size: 6.3 KB, free: 511.0 MB)
17/08/10 05:36:26 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1006
17/08/10 05:36:26 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[25] at map at BatchAccuracyAlgo.scala:121)
17/08/10 05:36:26 INFO cluster.YarnScheduler: Adding task set 8.0 with 1 tasks
17/08/10 05:36:26 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8, sandbox, partition 0,PROCESS_LOCAL, 2034 bytes)
17/08/10 05:36:26 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on sandbox:60885 (size: 6.3 KB, free: 511.1 MB)
17/08/10 05:36:26 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 65 ms on sandbox (1/1)
17/08/10 05:36:26 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool 
17/08/10 05:36:26 INFO scheduler.DAGScheduler: ResultStage 8 (take at LoggerPersist.scala:68) finished in 0.065 s
17/08/10 05:36:26 INFO scheduler.DAGScheduler: Job 2 finished: take at LoggerPersist.scala:68, took 0.077655 s
17/08/10 05:36:26 INFO spark.SparkContext: Starting job: take at LoggerPersist.scala:68
17/08/10 05:36:26 INFO scheduler.DAGScheduler: Got job 3 (take at LoggerPersist.scala:68) with 1 output partitions
17/08/10 05:36:26 INFO scheduler.DAGScheduler: Final stage: ResultStage 11 (take at LoggerPersist.scala:68)
17/08/10 05:36:26 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 9, ShuffleMapStage 10)
17/08/10 05:36:26 INFO scheduler.DAGScheduler: Missing parents: List()
17/08/10 05:36:26 INFO scheduler.DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[25] at map at BatchAccuracyAlgo.scala:121), which has no missing parents
17/08/10 05:36:26 INFO storage.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.0 KB, free 893.5 KB)
17/08/10 05:36:26 INFO storage.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 6.3 KB, free 899.7 KB)
17/08/10 05:36:26 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.17.0.2:35444 (size: 6.3 KB, free: 511.0 MB)
17/08/10 05:36:26 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1006
17/08/10 05:36:26 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[25] at map at BatchAccuracyAlgo.scala:121)
17/08/10 05:36:26 INFO cluster.YarnScheduler: Adding task set 11.0 with 1 tasks
17/08/10 05:36:26 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 9, sandbox, partition 1,PROCESS_LOCAL, 2034 bytes)
17/08/10 05:36:26 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on sandbox:60885 (size: 6.3 KB, free: 511.1 MB)
17/08/10 05:36:26 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 9) in 60 ms on sandbox (1/1)
17/08/10 05:36:26 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool 
Map($source['email'] -> tomajerrya044@dc.org, $source['post_code'] -> 94022, $source['phone'] -> 10000044, $source['last_name'] -> Jerrya, $source['address'] -> 244 DisneyCity, $source['first_name'] -> Tom049, $source['user_id'] -> 10044) [Map(__mismatch__ -> no target)]
Map($source['email'] -> tomajerrya050@dc.org, $source['post_code'] -> 94022, $source['phone'] -> 10000050, $source['last_name'] -> Jerrya, $source['address'] -> 250 DisneyCity, $source['first_name'] -> Tom0, $source['user_id'] -> 10050) [Map(__mismatch__ -> no target)]
Map($source['email'] -> tomajerrya049@dc.org, $source['post_code'] -> 94022, $source['phone'] -> 10000049, $source['last_name'] -> Jerrya, $source['address'] -> 249 DisneyCity, $source['first_name'] -> Tom049, $source['user_id'] -> 10050) [Map(__mismatch__ -> no target)]
Map($source['email'] -> tomajerrya039@dc.org, $source['post_code'] -> 94022, $source['phone'] -> 10000039, $source['last_name'] -> Jerrya, $source['address'] -> 239 DisneyCity123, $source['first_name'] -> Tom039, $source['user_id'] -> 10039) [Map(__mismatch__ -> no target)]
17/08/10 05:36:26 INFO scheduler.DAGScheduler: ResultStage 11 (take at LoggerPersist.scala:68) finished in 0.060 s
17/08/10 05:36:26 INFO scheduler.DAGScheduler: Job 3 finished: take at LoggerPersist.scala:68, took 0.071870 s
17/08/10 05:36:26 INFO spark.SparkContext: Starting job: count at HdfsPersist.scala:124
17/08/10 05:36:26 INFO scheduler.DAGScheduler: Got job 4 (count at HdfsPersist.scala:124) with 2 output partitions
17/08/10 05:36:26 INFO scheduler.DAGScheduler: Final stage: ResultStage 14 (count at HdfsPersist.scala:124)
17/08/10 05:36:26 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 12, ShuffleMapStage 13)
17/08/10 05:36:26 INFO scheduler.DAGScheduler: Missing parents: List()
17/08/10 05:36:26 INFO scheduler.DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[25] at map at BatchAccuracyAlgo.scala:121), which has no missing parents
17/08/10 05:36:26 INFO storage.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 14.8 KB, free 914.6 KB)
17/08/10 05:36:26 INFO storage.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 6.2 KB, free 920.8 KB)
17/08/10 05:36:26 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.17.0.2:35444 (size: 6.2 KB, free: 511.0 MB)
17/08/10 05:36:26 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1006
17/08/10 05:36:26 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 14 (MapPartitionsRDD[25] at map at BatchAccuracyAlgo.scala:121)
17/08/10 05:36:26 INFO cluster.YarnScheduler: Adding task set 14.0 with 2 tasks
17/08/10 05:36:26 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 10, sandbox, partition 0,PROCESS_LOCAL, 2034 bytes)
17/08/10 05:36:26 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 14.0 (TID 11, sandbox, partition 1,PROCESS_LOCAL, 2034 bytes)
17/08/10 05:36:26 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on sandbox:44160 (size: 6.2 KB, free: 511.1 MB)
17/08/10 05:36:26 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on sandbox:60885 (size: 6.2 KB, free: 511.0 MB)
17/08/10 05:36:26 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 10) in 61 ms on sandbox (1/2)
17/08/10 05:36:26 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 14.0 (TID 11) in 77 ms on sandbox (2/2)
17/08/10 05:36:26 INFO cluster.YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool 
17/08/10 05:36:26 INFO scheduler.DAGScheduler: ResultStage 14 (count at HdfsPersist.scala:124) finished in 0.079 s
17/08/10 05:36:26 INFO scheduler.DAGScheduler: Job 4 finished: count at HdfsPersist.scala:124, took 0.089297 s
17/08/10 05:36:26 INFO spark.SparkContext: Starting job: take at HdfsPersist.scala:129
17/08/10 05:36:26 INFO scheduler.DAGScheduler: Got job 5 (take at HdfsPersist.scala:129) with 1 output partitions
17/08/10 05:36:26 INFO scheduler.DAGScheduler: Final stage: ResultStage 17 (take at HdfsPersist.scala:129)
17/08/10 05:36:26 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 15, ShuffleMapStage 16)
17/08/10 05:36:26 INFO scheduler.DAGScheduler: Missing parents: List()
17/08/10 05:36:26 INFO scheduler.DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[25] at map at BatchAccuracyAlgo.scala:121), which has no missing parents
17/08/10 05:36:26 INFO storage.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 15.0 KB, free 935.8 KB)
17/08/10 05:36:26 INFO storage.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 6.3 KB, free 942.0 KB)
17/08/10 05:36:26 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.17.0.2:35444 (size: 6.3 KB, free: 511.0 MB)
17/08/10 05:36:26 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1006
17/08/10 05:36:26 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[25] at map at BatchAccuracyAlgo.scala:121)
17/08/10 05:36:26 INFO cluster.YarnScheduler: Adding task set 17.0 with 1 tasks
17/08/10 05:36:26 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 12, sandbox, partition 0,PROCESS_LOCAL, 2034 bytes)
17/08/10 05:36:26 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on sandbox:60885 (size: 6.3 KB, free: 511.0 MB)
17/08/10 05:36:26 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 12) in 61 ms on sandbox (1/1)
17/08/10 05:36:26 INFO cluster.YarnScheduler: Removed TaskSet 17.0, whose tasks have all completed, from pool 
17/08/10 05:36:26 INFO scheduler.DAGScheduler: ResultStage 17 (take at HdfsPersist.scala:129) finished in 0.062 s
17/08/10 05:36:26 INFO scheduler.DAGScheduler: Job 5 finished: take at HdfsPersist.scala:129, took 0.070852 s
17/08/10 05:36:26 INFO spark.SparkContext: Starting job: take at HdfsPersist.scala:129
17/08/10 05:36:26 INFO scheduler.DAGScheduler: Got job 6 (take at HdfsPersist.scala:129) with 1 output partitions
17/08/10 05:36:26 INFO scheduler.DAGScheduler: Final stage: ResultStage 20 (take at HdfsPersist.scala:129)
17/08/10 05:36:26 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 19, ShuffleMapStage 18)
17/08/10 05:36:26 INFO scheduler.DAGScheduler: Missing parents: List()
17/08/10 05:36:26 INFO scheduler.DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[25] at map at BatchAccuracyAlgo.scala:121), which has no missing parents
17/08/10 05:36:26 INFO storage.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 15.0 KB, free 957.0 KB)
17/08/10 05:36:26 INFO storage.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 6.3 KB, free 963.2 KB)
17/08/10 05:36:26 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.17.0.2:35444 (size: 6.3 KB, free: 511.0 MB)
17/08/10 05:36:26 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1006
17/08/10 05:36:26 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[25] at map at BatchAccuracyAlgo.scala:121)
17/08/10 05:36:26 INFO cluster.YarnScheduler: Adding task set 20.0 with 1 tasks
17/08/10 05:36:26 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 13, sandbox, partition 1,PROCESS_LOCAL, 2034 bytes)
17/08/10 05:36:26 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on sandbox:44160 (size: 6.3 KB, free: 511.1 MB)
17/08/10 05:36:26 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 13) in 77 ms on sandbox (1/1)
17/08/10 05:36:26 INFO scheduler.DAGScheduler: ResultStage 20 (take at HdfsPersist.scala:129) finished in 0.077 s
17/08/10 05:36:26 INFO cluster.YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool 
17/08/10 05:36:26 INFO scheduler.DAGScheduler: Job 6 finished: take at HdfsPersist.scala:129, took 0.100134 s
17/08/10 05:36:26 INFO persist.LoggerPersist: 1502343386747: persist using time: 651 ms
17/08/10 05:36:26 INFO persist.LoggerPersist: accu-sample finish
17/08/10 05:36:26 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/static/sql,null}
17/08/10 05:36:26 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/SQL/execution/json,null}
17/08/10 05:36:26 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/SQL/execution,null}
17/08/10 05:36:26 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/SQL/json,null}
17/08/10 05:36:26 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/SQL,null}
17/08/10 05:36:26 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
17/08/10 05:36:26 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
17/08/10 05:36:26 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}
17/08/10 05:36:26 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}
17/08/10 05:36:26 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}
17/08/10 05:36:26 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
17/08/10 05:36:26 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
17/08/10 05:36:26 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}
17/08/10 05:36:26 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}
17/08/10 05:36:26 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}
17/08/10 05:36:26 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}
17/08/10 05:36:26 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
17/08/10 05:36:26 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
17/08/10 05:36:26 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}
17/08/10 05:36:26 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}
17/08/10 05:36:26 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
17/08/10 05:36:26 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
17/08/10 05:36:26 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
17/08/10 05:36:26 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
17/08/10 05:36:26 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}
17/08/10 05:36:26 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}
17/08/10 05:36:26 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
17/08/10 05:36:26 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
17/08/10 05:36:26 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
17/08/10 05:36:26 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}
17/08/10 05:36:26 INFO ui.SparkUI: Stopped Spark web UI at http://172.17.0.2:4040
17/08/10 05:36:26 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors
17/08/10 05:36:26 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread
17/08/10 05:36:26 INFO cluster.YarnClientSchedulerBackend: Asking each executor to shut down
17/08/10 05:36:26 INFO cluster.YarnClientSchedulerBackend: Stopped
17/08/10 05:36:26 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/08/10 05:36:26 INFO storage.MemoryStore: MemoryStore cleared
17/08/10 05:36:26 INFO storage.BlockManager: BlockManager stopped
17/08/10 05:36:26 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
17/08/10 05:36:26 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/08/10 05:36:26 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
17/08/10 05:36:26 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
17/08/10 05:36:27 INFO spark.SparkContext: Successfully stopped SparkContext
17/08/10 05:36:27 INFO batch.Application$: calculation finished
17/08/10 05:36:27 INFO util.ShutdownHookManager: Shutdown hook called
17/08/10 05:36:27 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-9a7a7407-9038-4a80-bba5-dc8b7ea7dcfa/httpd-a3779ab6-54ec-4181-8c30-f3369889cce1
17/08/10 05:36:27 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-ab5b24e7-5b13-4d5d-9cd5-2e7103e2f427
17/08/10 05:36:27 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-9a7a7407-9038-4a80-bba5-dc8b7ea7dcfa
17/08/10 05:36:27 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
